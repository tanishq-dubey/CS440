<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>CS440 by tanishq-dubey</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>CS440</h1>
        <p>Written Assignments for CS440 - Artificial Intelligence at UIUC</p>

        <p class="view"><a href="https://github.com/tanishq-dubey/CS440">View the Project on GitHub <small>tanishq-dubey/CS440</small></a></p>


        <ul>
          <li><a href="https://github.com/tanishq-dubey/CS440/zipball/master">Download <strong>ZIP File</strong></a></li>
          <li><a href="https://github.com/tanishq-dubey/CS440/tarball/master">Download <strong>TAR Ball</strong></a></li>
          <li><a href="https://github.com/tanishq-dubey/CS440">View On <strong>GitHub</strong></a></li>
        </ul>
      </header>
      <section>
        <h1>
<a id="probablity" class="anchor" href="#probablity" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Probablity</h1>

<h2>
<a id="frequentist-vs-bayesian-statistics" class="anchor" href="#frequentist-vs-bayesian-statistics" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Frequentist VS Bayesian Statistics</h2>

<p><strong>Frequentist:</strong> Directly analyzing sample to reach probabilistic conclusion. Draws conclusions from sample data by analyzing the frequency or proportion of the data.</p>

<ul>
<li>World is imprecise</li>
<li>
<strong>Fuzzy Logic</strong> measures accuracy based on <em>degrees of seperation</em> from the truth instead of binary true or false</li>
</ul>

<p><strong>Bayesian Statistics:</strong> Using Bayes theorem to reach probabilistic conclusions and update the probability of a hypothesis as new information is gained.</p>

<ul>
<li>True state of the world is represented with "degrees of belief"</li>
<li>Evidence can be objective or subjective</li>
<li>Evidence can prove/disprove different distributions</li>
<li>Bayes Theorem to update probability</li>
</ul>

<h2>
<a id="random-variable-distribution-and-joints" class="anchor" href="#random-variable-distribution-and-joints" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Random Variable, Distribution, and Joints</h2>

<p><strong>Random Variable</strong> A variable whose value is a numerical outcome of a random phenomenon. Can be discrete or continuous. </p>

<ul>
<li>Discrete: Boolean, multi-valued, lots of possibilites, but always countable.</li>
<li>Continuous: Infinite possibilities for values, within a interval. </li>
</ul>

<p><strong>Probability Distribution</strong> A function of a discrete variable whose integral or summation over any interval is the probability that the random variable by it will like within that interval.</p>

<p><strong>Joint Probability Distribution (Full Joint)</strong> A function of multiple discrete variable whose integral or summation over any interval is the probability that the random variable specified by it will like within that area.</p>

<h2>
<a id="events" class="anchor" href="#events" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Events</h2>

<p><strong>Types:</strong> Atomic events, exclusive and exhaustive (possible end results of logic)</p>

<ul>
<li>Complex events are often a collection of atomic events</li>
</ul>

<p><strong>Underlying Population</strong> All possible sequences of events</p>

<p><strong>Sample</strong> A few observed sequences of events</p>

<h2>
<a id="distributions" class="anchor" href="#distributions" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Distributions</h2>

<p>Sum over the distribution must be 1 and every probability is a value greater than 0.</p>

<ul>
<li><em>P(A∨B) = P(A) + P(B) - P(A∧B)</em></li>
<li><em>P(A∧B) = P(A|B)P(B)</em></li>
<li><em>P(A∧B) = P(A,B)</em></li>
</ul>

<p>Conditionally independent if <em>P(A|B) = P(A)</em></p>

<h2>
<a id="bayes-theorem" class="anchor" href="#bayes-theorem" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Bayes Theorem</h2>

<p><em>P(A|B) = (P(B|A)P(A))/P(B)</em></p>

<h1>
<a id="bayesian-network" class="anchor" href="#bayesian-network" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Bayesian Network</h1>

<h2>
<a id="bn-graph-bn-cpts" class="anchor" href="#bn-graph-bn-cpts" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>BN graph, BN CPTs</h2>

<p><strong>Bayesian Net Graph</strong>: A Connected acyclic graph that models the conditional relationships between a system of random variables. If a system violates a bayesian network's conditional independence assumptions, then it cannot be modeled accurately.</p>

<ul>
<li>Conditional independence is shown in a bayes net with <em>Lack of an arc</em>
</li>
<li><strong><em>Nodes are random variables</em></strong></li>
<li><strong><em>Edges are directed influence (probability)</em></strong></li>
</ul>

<p><strong>Bayesian Net Conditional Probability Table:</strong> Bayesian Networks simplify the conditional relationships of a given situation relative to the full joint conditional probability tables. This drastically reduces the quantity of numbers that must be stored to accurately represent a given system. In Bayes net graphs, tables only need be stored for every node.</p>

<ul>
<li><strong><em>A table for each node representing the conditional probablities</em></strong></li>
</ul>

<p><strong>Conditional independence in Bayesian Networks:</strong> Represented in a bayesian networks by a <em>lack of an arc</em></p>

<p><strong>Constructing Bayesian Networks</strong></p>

<ul>
<li>From given knowledge of model, use arcs to connection variables</li>
<li>Go <em>from causes to symptoms</em> <strong><em>Ex. Smoking to lung cancer</em></strong>
</li>
</ul>

<p><strong>Inferences with Bayesian Nets</strong></p>

<ul>
<li>Using known evidence, traverse the given bayesian network, paying attention to probability.</li>
<li>Inference is using the probability of individual arcs to find the overall probability of a atomic event.</li>
</ul>

<p><em>A Bayesian network with CPTs for each node</em>
<img src="http://www.cs.ubc.ca/%7Emurphyk/Bayes/Figures/sprinkler.gif" alt="alt text" title="A Bayesian Network for  if a sprinkler should run"></p>

<p><strong>Non Poly Tree</strong> Bayesian networks with undirected cycles <em>There Are never directed cycles in a bayesian network</em></p>

<p><strong>Polytree:</strong> Bayesian networks with at most one undirected path between any two nodes</p>

<p><strong>Inferencing on a NonPolyTree</strong></p>

<ul>
<li>Joining trees, using a junction tree algorithm

<ul>
<li>Simplify a given tree by identifying clusters of nodes and forming super nodes</li>
</ul>
</li>
<li>Restore polytree property by modifying tree somehow</li>
<li><strong><em>Use a Montecarlo sampling algorithm</em></strong></li>
</ul>

<p><strong>Rejection Sampling</strong></p>

<ul>
<li>A Montecarlo method that involves producing samples and processing through a bayesian net rejecting or accepting them depending on their vale

<ul>
<li>Start with all RV's unknown</li>
<li>Choose a topological ordering</li>
<li>Get random numbers to assign values</li>
<li>Generate samples and reject anything that's not satisfied

<ul>
<li>Calculate probabilities by counting</li>
</ul>
</li>
</ul>
</li>
</ul>

<p><strong>Likelihood weighting</strong></p>

<ul>
<li>Fix evidence variables, sample only non-evidence variables and weight each sample by the likelihood it adheres to evidence

<ul>
<li>Assign a topological ordering</li>
<li>Generate a sample randomly</li>
<li>Reject nothing, weight each sample by its total likelihood</li>
</ul>
</li>
</ul>

<p><strong>Gibbs Sampling</strong></p>

<ul>
<li><strong><em>One version of a markov chain monte carlo algorithm</em></strong></li>
<li>Fix evidence variables to their values</li>
<li>Initialize all other values randomly</li>
<li>Iterate over all nodes many times</li>
<li>During each iteration resample the non-evidence variables using only the markov blaket for conditioning

<ul>
<li>Markov Blanket: Variable is conditionally independent of all other nodes given its parents, children and siblings</li>
</ul>
</li>
<li>Each iteration should adjust the probabilities slightly</li>
</ul>

<p><em>Naive Bayes</em>
            - A supervised learning algorithm based on applying Bayes' theorem with the naive assumption of independence between every pair of features. Only the most relevant relation is considered.
            - Usually, root is the query and leaves are the evidence nodes
            - Symptoms (evidence nodes) are conditionally independent of each other given the disease
            - Stringent assumptions, take the max relationship and use that as the likely relationship
            - lets you infer the most likely disease easily</p>

<h1>
<a id="machine-learning" class="anchor" href="#machine-learning" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Machine Learning</h1>

<h2>
<a id="example-and-hypothesis-space" class="anchor" href="#example-and-hypothesis-space" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Example and Hypothesis Space</h2>

<p><strong>Example Space</strong>: All possible inputs</p>

<p><strong>Hypothesis Space:</strong> All possible outputs (Concept Space)</p>

<h2>
<a id="decision-trees" class="anchor" href="#decision-trees" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Decision Trees</h2>

<p><strong>Decision Tree</strong> A problem solving tool that uses a tree-like graph to model all possible decisions and the resulting consequences of those decisions.</p>

<p><strong>Construction</strong> Given a set of samples:</p>

<ul>
<li>If a node is homogeneous or close to homogeneous: Stop</li>
<li>Else: Use the most useful test to split

<ul>
<li>Measure current entropy</li>
<li>Measure entropy after each possible split</li>
<li>Split on attribute that yields the lowest entropy</li>
<li><strong><em>Minimize Entropy, Maximize information gain</em></strong></li>
</ul>
</li>
<li>Recur on children

<ul>
<li>The algorithm will always halt</li>
<li>If attributes are continuous, discrete into ranges</li>
</ul>
</li>
<li>Simply:

<ul>
<li>Test: Get to a node, if good enough: STOP, else split</li>
<li>Split: Choose "most useful" test to split on</li>
<li>Feature: Recur on children</li>
</ul>
</li>
</ul>

<p><img src="https://upload.wikimedia.org/wikipedia/en/4/4f/GEP_decision_tree_with_numeric_and_nominal_attributes.png" alt="alt text" title="Tree displaying if we go golfing today"></p>

<h2>
<a id="entropy-pruning-overfitting-and-bias-variance-tradeoff" class="anchor" href="#entropy-pruning-overfitting-and-bias-variance-tradeoff" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Entropy, Pruning, Overfitting, and Bias Variance Tradeoff</h2>

<p><strong>Entropy</strong> Measure of disorder, used to measure information gain from a split</p>

<ul>
<li>H(s) = Sum(v){-P(v)log2(P(v))}</li>
<li>Split when information is greatest (<strong><em>Smallest Entropy</em></strong>)</li>
</ul>

<p><strong>Pruning</strong> A method used to reduce overfitting by reducing the size of a given decision tree, making it more general and thereby a better representation of the general problem. The last few splits on a decision tree often provide little information</p>

<ul>
<li>Alternatives to pruning

<ul>
<li>Restrict the expressiveness of H</li>
<li>Limit the number of nodes</li>
<li>Limit the height of the tree</li>
<li>Limit the minimum number of examples required for a split </li>
</ul>
</li>
</ul>

<p><strong>Overfitting:</strong> when a machine learning algorithm finds patterns that are only present in the training data set and are not representative of reality. This causes good performance on training data, but poor performance on testing data</p>

<ul>
<li>Need to stop training at a certain point with a high learning rate (relative) to avoid overfitting</li>
<li>Specifically in decision trees:

<ul>
<li>Split to nearly perfect data (homogeneous)</li>
<li>Few examples are at leaves</li>
<li>Low confidence in lower split choices</li>
<li>Reduced by limited the depth, set of tests, and minimum number of examples used to select a split</li>
</ul>
</li>
<li>Can overfit and Prune as well to correct local minima</li>
</ul>

<p><strong>Bias Variance Tradeoff:</strong> Given a limited training set, a decision tree model can either suffer from bias toward the training data due to overfitting or it can suffer from somewhat larger variance due to the more general nature of the resulting model.</p>

<h2>
<a id="bagging-and-cross-validation-variance-reduction" class="anchor" href="#bagging-and-cross-validation-variance-reduction" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Bagging and Cross Validation (Variance Reduction)</h2>

<p><strong>Cross Validation:</strong> Using a separate set of data for training and verifying a machine learning model</p>

<ul>
<li>Can combine results (averaging, etc)</li>
</ul>

<p><strong>Bagging:</strong> Split all of the data into <em>N</em> bags. Use <em>N-1</em> bags for training and 1 bag for testing. Iterate through all the combinations, average all models. Doing this drastically reduces the impact of overfitting.</p>

<ul>
<li>Grow and prune decision tree with each new addition</li>
<li>Split into <em>n</em> bags using <em>n-1</em> for training and 1 for testing.</li>
<li>Iterate and average</li>
</ul>

<h1>
<a id="learning-theory" class="anchor" href="#learning-theory" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Learning Theory</h1>

<h2>
<a id="simple-versus-agnostic-learning" class="anchor" href="#simple-versus-agnostic-learning" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Simple Versus Agnostic Learning</h2>

<p><strong>Simple Learning:</strong> Rote learning. Learning just to deal with the scenarios that are presented in training data. </p>

<ul>
<li>Finite Hypothesis Space

<ul>
<li>
<em>h</em> is a element of <em>H</em>
</li>
</ul>
</li>
<li>Find any <em>h</em> with zero empirical error</li>
</ul>

<p><strong>Agnostic Learning:</strong> Learning in a general form so that a given model can properly classify examples that exist outside of the training data space. </p>

<ul>
<li>Infinite Hypothesis Space</li>
<li>Find an <em>h</em> with a low empirical error</li>
</ul>

<h2>
<a id="finite-and-infinite-hypothesis-space" class="anchor" href="#finite-and-infinite-hypothesis-space" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Finite and Infinite Hypothesis Space</h2>

<p><strong>Finite Hypothesis Space:</strong> A bounded number of possible inputs and outputs</p>

<p><strong>Infinite Hypothesis Space:</strong> A unbounded number of inputs and outputs. Agnostic learning must be used in order to have a tractable model</p>

<h2>
<a id="computational-and-statistical-learning-bounds" class="anchor" href="#computational-and-statistical-learning-bounds" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Computational and Statistical Learning Bounds</h2>

<p><strong>Computational Learning Theory (CoLT):</strong> Determines the size of the required learning set by choosing <em>E</em> (acceptable error from ideal solutions), <em>D</em> (probably that the hypothesis is wrong) and <em>H</em> (the size of the hypothesis space).</p>

<ul>
<li>Determine size of the required learning set based off true error, delta, and <em>H</em>
</li>
</ul>

<p><strong>Statistical Learning Theory:</strong> given <em>D</em>, <em>H</em>, and <em>N</em> (size of the training set), determine the bound for the true error (<em>E</em>).</p>

<ul>
<li>Given input values for theory we can determine bound of true error (Best you can do based off your data and size)</li>
</ul>

<h2>
<a id="weak-law-of-large-numbers-chernoff-bounds" class="anchor" href="#weak-law-of-large-numbers-chernoff-bounds" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Weak Law of Large Numbers, Chernoff Bounds</h2>

<p><strong>Weak Law of Large Numbers:</strong> As the size of the training set approaches infinity, the probability of the difference of the expected output of a given hypothesis relative to the ideal approaches 0.</p>

<p>For infinite values of N, the Chernoff Concentration bound is used instead.</p>

<h2>
<a id="vapnik-chervonenskis-vc-dimension-and-shattering" class="anchor" href="#vapnik-chervonenskis-vc-dimension-and-shattering" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Vapnik-Chervonenskis (VC) Dimension and Shattering</h2>

<p>A measure of the capacity of a statistical classification algorithm</p>

<p><strong>Shattering:</strong> A classification model, <em>H</em>, is said to shatter a set of points, <em>N</em>, if and only if <em>H</em> can correctly label all of <em>N</em> for every combination of possible labels.</p>

<p><strong>Cardinality:</strong> Number of elements in a set</p>

<h3>
<a id="some-examples" class="anchor" href="#some-examples" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Some Examples:</h3>

<p><img src="http://i.imgur.com/fHeTvXL.png" alt="alt text" title="Example 1"></p>

<p><img src="http://i.imgur.com/OL1ltz5.png" alt="alt text" title="Example 2"></p>

<p><img src="http://i.imgur.com/bSOTj3m.png" alt="alt text" title="Example 3"></p>

<p><img src="http://i.imgur.com/xeTeUoN.png" alt="alt text" title="Example 4"></p>

<h2>
<a id="probably-approximately-correct-paclearnable" class="anchor" href="#probably-approximately-correct-paclearnable" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Probably Approximately Correct (PAC)Learnable</h2>

<p><strong>PAC Learning:</strong> A Method of machine learning that seeks to produce a hypothesis that with a high probability understands a target concept</p>

<ul>
<li>Concept class is PAC learnable if there exists an algorithm whose running time grows no faster than polynomially in natural complexity</li>
</ul>

<h1>
<a id="perceptrons" class="anchor" href="#perceptrons" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Perceptrons</h1>

<h2>
<a id="problem-a-single-perceptron-cannot-learn-xor" class="anchor" href="#problem-a-single-perceptron-cannot-learn-xor" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Problem: A Single Perceptron cannot learn XOR</h2>

<h2>
<a id="solution-artificial-neural-networks-logistic-regressions-and-support-vector-machines" class="anchor" href="#solution-artificial-neural-networks-logistic-regressions-and-support-vector-machines" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Solution: Artificial Neural Networks, Logistic Regressions, and Support Vector Machines</h2>

<p><strong>Artificial Neural Network:</strong> Machine learning model that was inspired by biological neural networks. It takes in a set of inputs and returns an output. </p>

<ul>
<li>Can represent any boolean function with one input layer</li>
<li>Purposefully using too few hidden nodes can be used to avoid overfitting</li>
</ul>

<p><strong>Logistic Regression:</strong> Essentially a simpler form of a artificial neural network that only uses one layer of linearly weighted inputs that are then passed through a sigmoid function</p>

<h2>
<a id="delta-learning-rule" class="anchor" href="#delta-learning-rule" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Delta Learning Rule</h2>

<ul>
<li>Cycle through each point one at a time</li>
<li>Only alter weights if perceptron makes a mistake</li>
<li>If the points are linearly separable, the algorithm will always converge and will always find a separator</li>
<li>If they are not linearly separable, the algorithm will not converge</li>
<li>Can accumulate <em>deltaW</em> changes for each sample prior to making a change. This results in a semi-gradient descent like behavior. 

<ul>
<li>
<strong>Epoch</strong> training does not maximize accuracy</li>
</ul>
</li>
</ul>

<h2>
<a id="gradient-descent-stochastic-gradient-descent" class="anchor" href="#gradient-descent-stochastic-gradient-descent" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Gradient Descent, Stochastic Gradient Descent</h2>

<p><strong>Gradient Descent</strong> A optimization algorithm that allows you to find local extrema. Each step is proportional to the gradient</p>

<ul>
<li>
<em>Gradient</em> is the direction of max increase (or max decrease)</li>
</ul>

<p>** Stochastic Gradient Descent** A form of gradient descent that looks at individual samples during each iteration instead of considering the whole set</p>

<h2>
<a id="back-propagation" class="anchor" href="#back-propagation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Back Propagation</h2>

<p><strong>Back Propagation</strong> Algorithm for updating ANN weights and biases based on error</p>

<h2>
<a id="margin" class="anchor" href="#margin" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Margin</h2>

<p><strong>Margin</strong> The distance from a single data point to a decision boundary</p>

<h2>
<a id="syntactic-versus-semantic-convergence" class="anchor" href="#syntactic-versus-semantic-convergence" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Syntactic versus Semantic Convergence</h2>

<p><strong>Syntactic Convergence</strong> When the values being considered converge. In the case of an ANN, this occurs when the weight and bias values converge</p>

<p><strong>Semantic Convergence</strong> When a given ML model converges to a state such that it accurately classifies new samples</p>

<h1>
<a id="features" class="anchor" href="#features" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Features</h1>

<h2>
<a id="bag-of-words" class="anchor" href="#bag-of-words" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Bag of Words</h2>

<p><strong>Bag of Words</strong> Representation of data commonly used in natural language processing that condenses a piece of data such as a sentence into a bag (multisite) of its fundamental components (words)</p>

<ul>
<li>The distance components and their multiplicities are stored.</li>
<li>
<strong>Issues</strong>

<ul>
<li>Does not work for phrases that use the same words rearranged</li>
<li>Count of words is not very meaningful</li>
<li>Longer documents have higher counts</li>
<li>Similar documents can look very different based on word choice</li>
</ul>
</li>
<li>
<strong>Solutions</strong>

<ul>
<li>Don't drop punctuation</li>
<li>Use TF-IDF</li>
<li>Convert everything to lowercase</li>
<li>Stop Words

<ul>
<li>Words that are too common to be discriminative, so should be removed</li>
</ul>
</li>
<li>Stemming

<ul>
<li>Combine variants of a word</li>
</ul>
</li>
<li>Synonym Pairing</li>
<li>Named Entity resolution</li>
<li>Include parts of speech </li>
</ul>
</li>
</ul>

<h2>
<a id="tf-idf" class="anchor" href="#tf-idf" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>TF-IDF</h2>

<p><strong>Term Frequency (TF) and Inverse Document Frequency(IDF)</strong> A numerical statistic that reflects the importance of a word to a document that is itself part of a larger corpus. The value increases proportionally to the number of times a word appears in a document but is offset by the frequency of the word in the entire corpus</p>

<ul>
<li>TF -&gt; Normalize the count, measure proportion</li>
<li>IDF -&gt; Use words as features</li>
</ul>

<p><em>Score = TF * IDF</em></p>

<p><em>TF(t,d) = N(t,d)/T(d)</em></p>

<p><em>IDF = ln(Total number of documents/ Number of Documents with t in it)</em></p>

<h2>
<a id="bag-of-codewords" class="anchor" href="#bag-of-codewords" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Bag of Codewords</h2>

<p>Multistep of clusters, sequences of normalized pixels usually that are unique to certain types of images. </p>

<h1>
<a id="unsupervised-learning" class="anchor" href="#unsupervised-learning" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Unsupervised Learning</h1>

<h2>
<a id="hierarchical-clustering-k-means" class="anchor" href="#hierarchical-clustering-k-means" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Hierarchical Clustering, K-Means</h2>

<p><strong>Hierarchical Clustering</strong> Method of cluster analysis that builds a hierarchy of clusters, works either "bottoms up" or "top down"</p>

<ul>
<li>Closest Elements</li>
<li>Farthest Elements </li>
<li>Median Elements</li>
<li>Centroid</li>
</ul>

<p><strong>K-Mean</strong> Set of algorithms that partitions <em>n</em> observations into <em>k</em> clusters. The mean of every element of a given cluster is closest to the model of a given cluster.</p>
      </section>
      <footer>
        <p>This project is maintained by <a href="https://github.com/tanishq-dubey">tanishq-dubey</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    
  </body>
</html>
