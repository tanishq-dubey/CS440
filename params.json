{
  "name": "CS440",
  "tagline": "Written Assignments for CS440 - Artificial Intelligence at UIUC",
  "body": "#Probablity\r\n\r\n##Frequentist VS Bayesian Statistics\r\n\r\n**Frequentist:** Directly analyzing sample to reach probabilistic conclusion. Draws conclusions from sample data by analyzing the frequency or proportion of the data.\r\n\r\n- World is imprecise\r\n- **Fuzzy Logic** measures accuracy based on _degrees of seperation_ from the truth instead of binary true or false\r\n\r\n**Bayesian Statistics:** Using Bayes theorem to reach probabilistic conclusions and update the probability of a hypothesis as new information is gained.\r\n\r\n- True state of the world is represented with \"degrees of belief\"\r\n- Evidence can be objective or subjective\r\n- Evidence can prove/disprove different distributions\r\n- Bayes Theorem to update probability\r\n\r\n##Random Variable, Distribution, and Joints\r\n\r\n**Random Variable** A variable whose value is a numerical outcome of a random phenomenon. Can be discrete or continuous. \r\n\r\n - Discrete: Boolean, multi-valued, lots of possibilites, but always countable.\r\n - Continuous: Infinite possibilities for values, within a interval. \r\n\r\n**Probability Distribution** A function of a discrete variable whose integral or summation over any interval is the probability that the random variable by it will like within that interval.\r\n\r\n**Joint Probability Distribution (Full Joint)** A function of multiple discrete variable whose integral or summation over any interval is the probability that the random variable specified by it will like within that area.\r\n\r\n##Events\r\n\r\n**Types:** Atomic events, exclusive and exhaustive (possible end results of logic)\r\n\r\n - Complex events are often a collection of atomic events\r\n\r\n**Underlying Population** All possible sequences of events\r\n\r\n**Sample** A few observed sequences of events\r\n\r\n##Distributions\r\n\r\nSum over the distribution must be 1 and every probability is a value greater than 0.\r\n\r\n - _P(A∨B) = P(A) + P(B) - P(A∧B)_\r\n - _P(A∧B) = P(A|B)P(B)_\r\n - _P(A∧B) = P(A,B)_\r\n\r\nConditionally independent if _P(A|B) = P(A)_\r\n\r\n##Bayes Theorem\r\n\r\n_P(A|B) = (P(B|A)P(A))/P(B)_\r\n\r\n\r\n#Bayesian Network\r\n\r\n##BN graph, BN CPTs\r\n\r\n**Bayesian Net Graph**: A Connected acyclic graph that models the conditional relationships between a system of random variables. If a system violates a bayesian network's conditional independence assumptions, then it cannot be modeled accurately.\r\n\r\n- Conditional independence is shown in a bayes net with _Lack of an arc_\r\n- **_Nodes are random variables_**\r\n- **_Edges are directed influence (probability)_**\r\n\r\n**Bayesian Net Conditional Probability Table:** Bayesian Networks simplify the conditional relationships of a given situation relative to the full joint conditional probability tables. This drastically reduces the quantity of numbers that must be stored to accurately represent a given system. In Bayes net graphs, tables only need be stored for every node.\r\n\r\n- **_A table for each node representing the conditional probablities_**\r\n\r\n**Conditional independence in Bayesian Networks:** Represented in a bayesian networks by a _lack of an arc_\r\n\r\n**Constructing Bayesian Networks**\r\n\r\n- From given knowledge of model, use arcs to connection variables\r\n- Go _from causes to symptoms_ **_Ex. Smoking to lung cancer_**\r\n\r\n\r\n**Inferences with Bayesian Nets**\r\n\r\n- Using known evidence, traverse the given bayesian network, paying attention to probability.\r\n- Inference is using the probability of individual arcs to find the overall probability of a atomic event.\r\n\r\n_A Bayesian network with CPTs for each node_\r\n![alt text][bnet]\r\n\r\n**Non Poly Tree** Bayesian networks with undirected cycles _There Are never directed cycles in a bayesian network_\r\n\t\t\t\r\n **Polytree:** Bayesian networks with at most one undirected path between any two nodes\r\n\r\n**Inferencing on a NonPolyTree**\r\n\r\n- Joining trees, using a junction tree algorithm\r\n - Simplify a given tree by identifying clusters of nodes and forming super nodes\r\n- Restore polytree property by modifying tree somehow\r\n- **_Use a Montecarlo sampling algorithm_**\r\n\r\n**Rejection Sampling**\r\n\r\n- A Montecarlo method that involves producing samples and processing through a bayesian net rejecting or accepting them depending on their vale\r\n\t- Start with all RV's unknown\r\n\t- Choose a topological ordering\r\n\t- Get random numbers to assign values\r\n\t- Generate samples and reject anything that's not satisfied\r\n\t - Calculate probabilities by counting\r\n\r\n**Likelihood weighting**\r\n\r\n- Fix evidence variables, sample only non-evidence variables and weight each sample by the likelihood it adheres to evidence\r\n\t- Assign a topological ordering\r\n\t- Generate a sample randomly\r\n\t- Reject nothing, weight each sample by its total likelihood\r\n\r\n**Gibbs Sampling**\r\n\r\n- **_One version of a markov chain monte carlo algorithm_**\r\n- Fix evidence variables to their values\r\n- Initialize all other values randomly\r\n- Iterate over all nodes many times\r\n- During each iteration resample the non-evidence variables using only the markov blaket for conditioning\r\n\t - Markov Blanket: Variable is conditionally independent of all other nodes given its parents, children and siblings\r\n- Each iteration should adjust the probabilities slightly\r\n\r\n*Naive Bayes*\r\n\t\t\t- A supervised learning algorithm based on applying Bayes' theorem with the naive assumption of independence between every pair of features. Only the most relevant relation is considered.\r\n\t\t\t- Usually, root is the query and leaves are the evidence nodes\r\n\t\t\t- Symptoms (evidence nodes) are conditionally independent of each other given the disease\r\n\t\t\t- Stringent assumptions, take the max relationship and use that as the likely relationship\r\n\t\t\t- lets you infer the most likely disease easily\r\n\r\n#Machine Learning\r\n\r\n##Example and Hypothesis Space\r\n\r\n**Example Space**: All possible inputs\r\n\r\n**Hypothesis Space:** All possible outputs (Concept Space)\r\n\t\r\n##Decision Trees\r\n\r\n**Decision Tree** A problem solving tool that uses a tree-like graph to model all possible decisions and the resulting consequences of those decisions.\r\n\r\n**Construction** Given a set of samples:\r\n\r\n- If a node is homogeneous or close to homogeneous: Stop\r\n- Else: Use the most useful test to split\r\n\t- Measure current entropy\r\n\t- Measure entropy after each possible split\r\n\t- Split on attribute that yields the lowest entropy\r\n\t- **_Minimize Entropy, Maximize information gain_**\r\n- Recur on children\r\n\t- The algorithm will always halt\r\n\t- If attributes are continuous, discrete into ranges\r\n- Simply:\r\n\t- Test: Get to a node, if good enough: STOP, else split\r\n\t- Split: Choose \"most useful\" test to split on\r\n\t- Feature: Recur on children\r\n\r\n![alt text][dtree]\r\n\r\n##Entropy, Pruning, Overfitting, and Bias Variance Tradeoff\r\n\r\n**Entropy** Measure of disorder, used to measure information gain from a split\r\n\r\n- H(s) = Sum(v){-P(v)log2(P(v))}\r\n- Split when information is greatest (**_Smallest Entropy_**)\r\n\r\n**Pruning** A method used to reduce overfitting by reducing the size of a given decision tree, making it more general and thereby a better representation of the general problem. The last few splits on a decision tree often provide little information\r\n\r\n- Alternatives to pruning\r\n\t- Restrict the expressiveness of H\r\n\t- Limit the number of nodes\r\n\t- Limit the height of the tree\r\n\t- Limit the minimum number of examples required for a split \r\n\r\n**Overfitting:** when a machine learning algorithm finds patterns that are only present in the training data set and are not representative of reality. This causes good performance on training data, but poor performance on testing data\r\n\r\n- Need to stop training at a certain point with a high learning rate (relative) to avoid overfitting\r\n- Specifically in decision trees:\r\n - Split to nearly perfect data (homogeneous)\r\n - Few examples are at leaves\r\n - Low confidence in lower split choices\r\n - Reduced by limited the depth, set of tests, and minimum number of examples used to select a split\r\n- Can overfit and Prune as well to correct local minima\r\n\r\n**Bias Variance Tradeoff:** Given a limited training set, a decision tree model can either suffer from bias toward the training data due to overfitting or it can suffer from somewhat larger variance due to the more general nature of the resulting model.\r\n\r\n##Bagging and Cross Validation (Variance Reduction)\r\n\r\n**Cross Validation:** Using a separate set of data for training and verifying a machine learning model\r\n\r\n- Can combine results (averaging, etc)\r\n\r\n**Bagging:** Split all of the data into _N_ bags. Use _N-1_ bags for training and 1 bag for testing. Iterate through all the combinations, average all models. Doing this drastically reduces the impact of overfitting.\r\n\r\n- Grow and prune decision tree with each new addition\r\n- Split into _n_ bags using _n-1_ for training and 1 for testing.\r\n- Iterate and average\r\n\r\n#Learning Theory\r\n##Simple Versus Agnostic Learning\r\n\r\n**Simple Learning:** Rote learning. Learning just to deal with the scenarios that are presented in training data. \r\n\r\n- Finite Hypothesis Space\r\n - _h_ is a element of _H_\r\n- Find any _h_ with zero empirical error\r\n\r\n**Agnostic Learning:** Learning in a general form so that a given model can properly classify examples that exist outside of the training data space. \r\n\r\n - Infinite Hypothesis Space\r\n - Find an _h_ with a low empirical error\r\n\r\n##Finite and Infinite Hypothesis Space\r\n\r\n**Finite Hypothesis Space:** A bounded number of possible inputs and outputs\r\n\r\n**Infinite Hypothesis Space:** A unbounded number of inputs and outputs. Agnostic learning must be used in order to have a tractable model\r\n\r\n##Computational and Statistical Learning Bounds\r\n**Computational Learning Theory (CoLT):** Determines the size of the required learning set by choosing _E_ (acceptable error from ideal solutions), _D_ (probably that the hypothesis is wrong) and _H_ (the size of the hypothesis space).\r\n\r\n - Determine size of the required learning set based off true error, delta, and _H_\r\n\r\n**Statistical Learning Theory:** given _D_, _H_, and _N_ (size of the training set), determine the bound for the true error (_E_).\r\n\r\n - Given input values for theory we can determine bound of true error (Best you can do based off your data and size)\r\n\r\n##Weak Law of Large Numbers, Chernoff Bounds\r\n**Weak Law of Large Numbers:** As the size of the training set approaches infinity, the probability of the difference of the expected output of a given hypothesis relative to the ideal approaches 0.\r\n\r\nFor infinite values of N, the Chernoff Concentration bound is used instead.\r\n\r\n##Vapnik-Chervonenskis (VC) Dimension and Shattering\r\n \r\nA measure of the capacity of a statistical classification algorithm\r\n\r\n**Shattering:** A classification model, _H_, is said to shatter a set of points, _N_, if and only if _H_ can correctly label all of _N_ for every combination of possible labels.\r\n\r\n**Cardinality:** Number of elements in a set\r\n\r\n###Some Examples:\r\n\r\n![alt text][ex1]\r\n\r\n![alt text][ex2]\r\n\r\n![alt text][ex3]\r\n\r\n![alt text][ex4]\r\n\r\n##Probably Approximately Correct (PAC)Learnable\r\n\r\n**PAC Learning:** A Method of machine learning that seeks to produce a hypothesis that with a high probability understands a target concept\r\n\r\n - Concept class is PAC learnable if there exists an algorithm whose running time grows no faster than polynomially in natural complexity\r\n\r\n#Perceptrons\r\n##Problem: A Single Perceptron cannot learn XOR\r\n##Solution: Artificial Neural Networks, Logistic Regressions, and Support Vector Machines\r\n\r\n**Artificial Neural Network:** Machine learning model that was inspired by biological neural networks. It takes in a set of inputs and returns an output. \r\n\r\n - Can represent any boolean function with one input layer\r\n - Purposefully using too few hidden nodes can be used to avoid overfitting\r\n\r\n**Logistic Regression:** Essentially a simpler form of a artificial neural network that only uses one layer of linearly weighted inputs that are then passed through a sigmoid function\r\n\r\n##Delta Learning Rule\r\n\r\n - Cycle through each point one at a time\r\n - Only alter weights if perceptron makes a mistake\r\n - If the points are linearly separable, the algorithm will always converge and will always find a separator\r\n - If they are not linearly separable, the algorithm will not converge\r\n - Can accumulate _deltaW_ changes for each sample prior to making a change. This results in a semi-gradient descent like behavior. \r\n  - **Epoch** training does not maximize accuracy\r\n\r\n##Gradient Descent, Stochastic Gradient Descent\r\n\r\n**Gradient Descent** A optimization algorithm that allows you to find local extrema. Each step is proportional to the gradient\r\n\r\n - _Gradient_ is the direction of max increase (or max decrease)\r\n\r\n** Stochastic Gradient Descent** A form of gradient descent that looks at individual samples during each iteration instead of considering the whole set\r\n\r\n##Back Propagation\r\n\r\n**Back Propagation** Algorithm for updating ANN weights and biases based on error\r\n\r\n##Margin\r\n\r\n**Margin** The distance from a single data point to a decision boundary\r\n\r\n## Syntactic versus Semantic Convergence\r\n\r\n**Syntactic Convergence** When the values being considered converge. In the case of an ANN, this occurs when the weight and bias values converge\r\n\r\n**Semantic Convergence** When a given ML model converges to a state such that it accurately classifies new samples\r\n\r\n#Features\r\n##Bag of Words\r\n**Bag of Words** Representation of data commonly used in natural language processing that condenses a piece of data such as a sentence into a bag (multisite) of its fundamental components (words)\r\n\r\n - The distance components and their multiplicities are stored.\r\n - **Issues**\r\n  - Does not work for phrases that use the same words rearranged\r\n  - Count of words is not very meaningful\r\n  - Longer documents have higher counts\r\n  - Similar documents can look very different based on word choice\r\n - **Solutions**\r\n  - Don't drop punctuation\r\n  - Use TF-IDF\r\n  - Convert everything to lowercase\r\n  - Stop Words\r\n    - Words that are too common to be discriminative, so should be removed\r\n  - Stemming\r\n    - Combine variants of a word\r\n  - Synonym Pairing\r\n  - Named Entity resolution\r\n  - Include parts of speech \r\n\r\n##TF-IDF\r\n**Term Frequency (TF) and Inverse Document Frequency(IDF)** A numerical statistic that reflects the importance of a word to a document that is itself part of a larger corpus. The value increases proportionally to the number of times a word appears in a document but is offset by the frequency of the word in the entire corpus\r\n - TF -> Normalize the count, measure proportion\r\n - IDF -> Use words as features\r\n\r\n_Score = TF * IDF_\r\n\r\n_TF(t,d) = N(t,d)/T(d)_\r\n\r\n_IDF = ln(Total number of documents/ Number of Documents with t in it)_\r\n\r\n##Bag of Codewords\r\n\r\nMultistep of clusters, sequences of normalized pixels usually that are unique to certain types of images. \r\n\r\n#Unsupervised Learning\r\n##Hierarchical Clustering, K-Means\r\n\r\n**Hierarchical Clustering** Method of cluster analysis that builds a hierarchy of clusters, works either \"bottoms up\" or \"top down\"\r\n\r\n - Closest Elements\r\n - Farthest Elements \r\n - Median Elements\r\n - Centroid\r\n\r\n**K-Mean** Set of algorithms that partitions _n_ observations into _k_ clusters. The mean of every element of a given cluster is closest to the model of a given cluster.\r\n\r\n[ex1]: http://i.imgur.com/fHeTvXL.png \"Example 1\"\r\n[ex2]: http://i.imgur.com/OL1ltz5.png \"Example 2\"\r\n[ex3]: http://i.imgur.com/bSOTj3m.png \"Example 3\"\r\n[ex4]: http://i.imgur.com/xeTeUoN.png \"Example 4\"\r\n[dtree]: https://upload.wikimedia.org/wikipedia/en/4/4f/GEP_decision_tree_with_numeric_and_nominal_attributes.png \"Tree displaying if we go golfing today\"\r\n[bnet]: http://www.cs.ubc.ca/~murphyk/Bayes/Figures/sprinkler.gif \"A Bayesian Network for  if a sprinkler should run\"\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}