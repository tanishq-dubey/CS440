{
  "name": "CS440",
  "tagline": "Written Assignments for CS440 - Artificial Intelligence at UIUC",
  "body": "# CS440\r\nWritten Assignments for CS440 - Artificial Intelligence at UIUC\r\n\r\nCoding portions will be uploaded at a later date, once I have recovered them.\r\n\r\nAs of right now, pre-rendered versions assignments can be accessed here:\r\n\r\n * [Assignment 1](https://github.com/tanishq-dubey/CS440/blob/master/CS%20440%20HW%201/CS_440_HW_1.pdf)\r\n * [Assignment 2](https://github.com/tanishq-dubey/CS440/blob/master/CS%20440%20HW%202/CS_440_HW_2.pdf)\r\n * [Assignment 3](https://github.com/tanishq-dubey/CS440/blob/master/CS%20440%20HW%203/CS_440_HW_3.pdf)\r\n * [Assignment 4](https://github.com/tanishq-dubey/CS440/blob/master/CS%20440%20HW%204/CS_440_HW_4.pdf)\r\n * [Assignment 5](https://github.com/tanishq-dubey/CS440/blob/master/CS%20440%20HW%205/CS_440_HW_5.pdf)\r\n * [Assignment 6](https://github.com/tanishq-dubey/CS440/blob/master/CS%20440%20HW%206/CS_440_HW_6.pdf)\r\n * [Assignment 7](https://github.com/tanishq-dubey/CS440/blob/master/CS%20440%20HW%207/CS_440_HW_7.pdf)\r\n\r\nRaw LaTeX can be found on the [repo](https://github.com/tanishq-dubey/CS440).\r\n\r\n#A Study Guide for the Final:\r\n\r\n#Probablity\r\n\r\n##Frequentist VS Bayesian Statistics\r\n\r\n**Frequentist:** Directly analyzing sample to reach probabilistic conclusion\r\n\r\n- World is imprecise\r\n- **Fuzzy Logic** measures accuracy based on _degrees of seperation_ from the truth instead of binary true or false\r\n\r\n**Bayesian Statistics:** Using Bayes theorem to reach probabilistic conclusions\r\n\r\n- True state of the world is represented with \"degrees of belief\"\r\n- Evidence can be objective or subjective\r\n- Evidence can prove/disprove different distributions\r\n- Bayes Theorem to update probablity\r\n\r\n##Random Variable, Distribution, Joint\r\n\r\n\r\n\r\n#Bayesian Network\r\n\r\n##BN graph, BN CPTs\r\n\r\n**Bayesian Net Graph**: A Connected acyclic graph that models the condidtional relationships between a system of random variables. If a system violates a bayesian network's conditional independence assumptions, then it cannot be modeled accurately.\r\n\r\n- Conditional independence is shown in a bayes net with _Lack of an arc_\r\n- **_Nodes are random variables_**\r\n- **_Edges are directed influence (probability)_**\r\n\r\n**Bayesian Net Conditional Proablity Table:** Bayesian Networks simplify the conditional relationships of a given situation relative to the full joint conditional probablity tables. This drastically reduces the quantity of numbers that must be stored to accurately represent a given system. In Bayes net graphs, tables only need be stored for every node.\r\n\r\n- **_A table for each node representing the conditional probablities_**\r\n\r\n**Conditional independence in Bayesian Networks:** Represented in a bayesian netowrks by a _lack of an arc_\r\n\r\n**Constructing Bayesian Networks**\r\n\r\n- From given knowledge of model, use arcs to connection variables\r\n- Go _from causes to symptoms_ **_Ex. Smoking to lung cancer_**\r\n\r\n\r\n**Inferences with Bayesian Nets**\r\n\r\n- Using known evidence, traverse the given bayesian network, paying attention to probablity.\r\n- Inference is using the probablity of individual arcs to find the overall probablity of a atomic event.\r\n\r\n**Non Poly Tree** Bayesian networks with undirected cycles _There Are never directed cycles in a bayesian network_\r\n\t\t\t\r\n **Polytree:** Bayesian networks with at most one undirected path between any two nodes\r\n\r\n**Inferencing on a NonPolyTree**\r\n\r\n- Joining trees, using a junction tree algorithm\r\n - Simplify a given tree by identifying clusters of nodes and forming super nodes\r\n- Restore polytree property by modifying tree somehow\r\n- **_Use a montecarlo sampling algorithm_**\r\n\r\n**Rejection Sampling**\r\n\r\n- A montecarlo method that involves producing samples and processing through a bayesian net rejecting or accepting them depending on their vale\r\n\t- Start with all RV's unknown\r\n\t- Choose a topological ordering\r\n\t- Get random numbers to assign values\r\n\t- Generate samples and reject anythig that's not satisfied\r\n\t - Calculate probablities by counting\r\n\r\n**Likelihood weighting**\r\n\r\n- Fix evidence variables, sample only nonevidence variables and wieght each sample by the likelihood it adhears to evidence\r\n\t- Assign a topological ordering\r\n\t- Generate a sample randomly\r\n\t- Reject nothing, weight each sample by its total likelihood\r\n\r\n**Gibbs Sampling**\r\n\r\n- **_One version of a markov chain monte carlo algorithm_**\r\n- Fix evidence variables to their values\r\n- Initalize all other values randomly\r\n- Iterate over all nodes many times\r\n- During each iteration resample the non-evidence variables using only the markov blaket for conditioning\r\n\t - Markov Blanket: Variable is conditionally independent of all other nodes given its parents, children and siblings\r\n- Each iteration should adjuct the probablities slightly\r\n\r\n*Naive Bayes*\r\n\t\t\t- A supervised learning algorithm based on applying Bayes' theorem with the naive assumption of independence between every pair of features. Only the most relevant relation is considered.\r\n\t\t\t- Ususally, root is the query and leaves are the evidence nodes\r\n\t\t\t- Symptoms (evidence nodes) are conditionally independent of each other given the disease\r\n\t\t\t- Stringent assumptions, take the max relationship and use that as the likely relationsship\r\n\t\t\t- lets you infer the most likely disease easily\r\n\r\n#Machine Learning\r\n\r\n##Example and Hypothesis Space\r\n\r\n**Example Space**: All possible inputs\r\n\r\n**Hypothesis Space:** All possible outputs (Concept Space)\r\n\t\r\n##Decision Trees\r\n\r\n**Decision Tree** A problem solving tool that uses a tree-like graph to model all possible decisions and the resulting consequences of those decisions.\r\n\r\n**Construction** Given a set of samples:\r\n\r\n- If a node is homogeneous or close to homogeneous: Stop\r\n- Else: Use the most useful test to split\r\n\t- Measure current entropy\r\n\t- Measure entropy after eaech possible split\r\n\t- Split on attribute that yeilds the lowest entropy\r\n\t- **_Minimize Entropy, Maximize information gain_**\r\n- Recur on children\r\n\t- The algorithm will always halt\r\n\t- If attributes are continuous, discrete into ranges\r\n- Simply:\r\n\t- Test: Get to a node, if good enough: STOP, else split\r\n\t- Split: Choose \"most useful\" test to split on\r\n\t- Feature: Recur on children\r\n\r\n##Entropy, Pruing, Overfitting, and Bias Variance Tradeoff\r\n\r\n**Entropy** Measure of disorder, used to masure information gain from a split\r\n\r\n- H(s) = Sum(v){-P(v)log2(P(v))}\r\n- Split when information is greatest (**_Smallest Entropy_**)\r\n\r\n**Pruning** A method used to reduce overfitting by reducing the size of a given decision tree, making it more general and thereby a better representation of the general problem. The last few splits on a decision tree often provide little information\r\n\r\n- Alternatives to pruning\r\n\t- Restrict the expressiveness of H\r\n\t- Limit the number of nodes\r\n\t- Limit the height of the tree\r\n\t- Limit the minimum number of examples required for a split \r\n\r\n**Overfitting:** when a machine learning algorithm finds patterns that are only present in the training data set and are not representative of reality. This causes good performance on training data, but poor performance on testing data\r\n\r\n- Need to stop training at a certain point with a high learning rate (relative) to avoid overfitting\r\n- Specifically in decision trees:\r\n - Split to nearly perfect data (homogeneous)\r\n - Few examples are at leaves\r\n - Low confidence in lower split choices\r\n - Reduced by limited the depth, set of tests, and minimum number of examples used to select a split\r\n- Can overfit and Prune as well to correct local minima\r\n\r\n**Bias Variance Tradeoff:** Given a limited training set, a decision tree model can either suffer from bias toward the training data due to overfitting or it can suffer from somewhat larger variance due to the more general nature of the resulting model.\r\n\r\n##Bagging and Cross Validation (Variance Reduction)\r\n\r\n**Cross Validation:** Using a separate set of data for training and verifying a machine learning model\r\n\r\n- Can combine results (averaging, etc)\r\n\r\n**Bagging:** Split all of the data into _N_ bags. Use _N-1_ bags for training and 1 bag for testing. Iterate through all the combinations, average all models. Doing this drastically reduces the impact of overfitting.\r\n\r\n- Grow and prune decision tree with each new addition\r\n- Split into _n_ bags using _n-1_ for training and 1 for testing.\r\n- Iterate and average\r\n\r\n#Learning Theory\r\n##Simple Versus Agnostic Learning\r\n\r\n**Simple Learning:** Rote learning. Learning just to deal with the scenarios that are presented in training data. \r\n\r\n- Finite Hypothesis Space\r\n - _h_ is a element of _H_\r\n- Find any _h_ with zero empirical error\r\n\r\n**Agnostic Learning:** Learning in a general form so that a given model can properly classify examples that exist outside of the training data space. \r\n\r\n - Infinite Hypothesis Space\r\n - Find an _h_ with a low empirical error\r\n\r\n##Finite and Infite Hypothesis Space\r\n\r\n**Finite Hypothesis Space:** A bounded number of possible inputs and outputs\r\n\r\n**Infinite Hypothesis Space:** A unbounded number of inputs and outputs. Agnostic learning must be used in order to have a tractable model\r\n\r\n##Computational and Statistical Learning Bounds\r\n**Computational Learning Theory (CoLT):** Determines the size of the required learning set by choosing _E_ (acceptable error from ideal solutions), _D_ (probably that the hypothesis is wrong) and _H_ (the size of the hypothesis space).\r\n\r\n - Determine size of the required learning set based off true error, delta, and _H_\r\n\r\n**Statistical Learning Theory:** given _D_, _H_, and _N_ (size of the training set), determine the bound for the true error (_E_).\r\n\r\n - Given input values for theory we can determine bound of true error (Best you can do based off your data and size)\r\n\r\n##Weak Law of Large Numbers, Chernoff Bounds\r\n**\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}